{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define F1 score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pairwise_operator(codes, method):\n",
    "    \"\"\"\n",
    "    Pairwsise operator based on a method and a list of predictions (e.g., lists of offsets)\n",
    "    >>> assert pairwise_operator([[],[],[]], f1) == 1\n",
    "    :param codes: a list of lists of predicted offsets\n",
    "    :param method: a method to use to compare all pairs\n",
    "    :return: the mean score between all possible pairs (excl. duplicates)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i,coderi in enumerate(codes):\n",
    "        for j,coderj in enumerate(codes):\n",
    "            if j>i:\n",
    "                pairs.append(method(coderi, coderj))\n",
    "    return np.mean(pairs)\n",
    "\n",
    "\n",
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
    "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
    "    :param predictions: a list of predicted offsets\n",
    "    :param gold: a list of offsets serving as the ground truth\n",
    "    :return: a score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1 if len(predictions)==0 else 0\n",
    "    nom = 2*len(set(predictions).intersection(set(gold)))\n",
    "    denom = len(set(predictions))+len(set(gold))\n",
    "    return nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train data\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"tsd_test.csv\")\n",
    "texts = list(train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[81, 82, 83, 84, 85, 86]</td>\n",
       "      <td>\"Watch people die from taking away their healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>tens years ago i contacted the PDR and suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>The parallels between the ANC and the Sicilian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>Intel Community: ‘How can we work for a Presid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               spans  \\\n",
       "0  [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...   \n",
       "1                           [81, 82, 83, 84, 85, 86]   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                                text  \n",
       "0  That's right. They are not normal. And I am st...  \n",
       "1  \"Watch people die from taking away their healt...  \n",
       "2  tens years ago i contacted the PDR and suggest...  \n",
       "3  The parallels between the ANC and the Sicilian...  \n",
       "4  Intel Community: ‘How can we work for a Presid...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ground-truth spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of list with character offsets of dataset\n",
    "\n",
    "spans = []\n",
    "\n",
    "for i, row in train.iterrows():\n",
    "    to_append_list = [int(x) for x in str(train[\"spans\"].iloc[i])[1:len(str(train[\"spans\"].iloc[i]))-1].split(\", \") if len(x) > 0]\n",
    "    spans.append(to_append_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group non-contiguous spans\n",
    "\n",
    "import more_itertools as mit\n",
    "span_grouped = []\n",
    "\n",
    "for i in range(len(spans)):\n",
    "    span_grouped.append([list(group) for group in mit.consecutive_groups(spans[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find starting and ending offset of non-contiguos offset\n",
    "\n",
    "\n",
    "final = []\n",
    "\n",
    "for i in range(len(span_grouped)):\n",
    "    span_grouped_just_group_sentence = []\n",
    "    for ranges in span_grouped[i]:\n",
    "        span_grouped_just_group_groups = []\n",
    "        #print(ranges)\n",
    "        if len(ranges) > 0:\n",
    "            span_grouped_just_group_groups = [ranges[0],ranges[-1]]\n",
    "        \n",
    "        span_grouped_just_group_sentence.append(span_grouped_just_group_groups)\n",
    "        \n",
    "    final.append(span_grouped_just_group_sentence)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_spans = [item for sublist in final for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground-truth spans\n",
    "\n",
    "only_spans = []\n",
    "\n",
    "for i in range(len(final)):\n",
    "    sentence_spans = []\n",
    "    for j in range(len(final[i])):\n",
    "        text = texts[i][final[i][j][0]:final[i][j][1]+1]\n",
    "        sentence_spans.append(text)\n",
    "    only_spans.append(sentence_spans)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in only_spans for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_length = []\n",
    "count_span = 0\n",
    "for sentence_span in only_spans:\n",
    "    sentence_span_length = []\n",
    "    for span in sentence_span:\n",
    "        sentence_span_length.append(len(span.split(\" \")))\n",
    "    all_length.append(sentence_span_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract predicted spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read predicted spans\n",
    "\n",
    "with open(\"spans-pred-biaffine.txt\", 'r') as file1:\n",
    "    lines = file1.readlines()\n",
    "\n",
    "all_answers = []\n",
    "\n",
    "for line in lines:\n",
    "    if not (line.isspace()):\n",
    "        feats = eval(line.split('\\t')[1])\n",
    "        all_answers.append(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of list with character offsets of dataset\n",
    "\n",
    "spans_predicted = []\n",
    "\n",
    "for i in range(len(all_answers)):\n",
    "    to_append_list = [int(x) for x in str(all_answers[i])[1:len(str(all_answers[i]))-1].split(\", \") if len(x) > 0]\n",
    "    spans_predicted.append(to_append_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group non-contiguous spans\n",
    "\n",
    "import more_itertools as mit\n",
    "span_grouped_predicted = []\n",
    "\n",
    "for i in range(len(spans_predicted)):\n",
    "    span_grouped_predicted.append([list(group) for group in mit.consecutive_groups(spans_predicted[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find starting and ending offset of non-contiguos offset\n",
    "\n",
    "final_predicted = []\n",
    "\n",
    "for i in range(len(span_grouped_predicted)):\n",
    "    span_grouped_just_group_sentence = []\n",
    "    for ranges in span_grouped_predicted[i]:\n",
    "        span_grouped_just_group_groups = []\n",
    "        #print(ranges)\n",
    "        if len(ranges) > 0:\n",
    "            span_grouped_just_group_groups = [ranges[0],ranges[-1]]\n",
    "        \n",
    "        span_grouped_just_group_sentence.append(span_grouped_just_group_groups)\n",
    "        \n",
    "    final_predicted.append(span_grouped_just_group_sentence)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predicted spans\n",
    "\n",
    "only_spans_predicted = []\n",
    "\n",
    "for i in range(len(final_predicted)):\n",
    "    sentence_spans = []\n",
    "    for j in range(len(final_predicted[i])):\n",
    "        text = texts[i][final_predicted[i][j][0]:final_predicted[i][j][1]+1]\n",
    "        sentence_spans.append(text)\n",
    "    only_spans_predicted.append(sentence_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length Calculation\n",
    "\n",
    "all_length_predicted = []\n",
    "count_span = 0\n",
    "for sentence_span in only_spans_predicted:\n",
    "    sentence_span_length = []\n",
    "    for span in sentence_span:\n",
    "        sentence_span_length.append(len(span.split(\" \")))\n",
    "    all_length_predicted.append(sentence_span_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Filters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of top 50 occuring cuss words (single word)\n",
    "\n",
    "tox_dict = {'Damn': 34,\n",
    " 'Dumb': 20,\n",
    " 'IDIOT': 16,\n",
    " 'Idiot': 41,\n",
    " 'Idiots': 38,\n",
    " 'Pathetic': 27,\n",
    " 'Ridiculous': 15,\n",
    " 'STUPID': 32,\n",
    " 'Stupid': 82,\n",
    " 'arrogant': 19,\n",
    " 'ass': 55,\n",
    " 'asshole': 20,\n",
    " 'bitch': 19,\n",
    " 'buffoon': 24,\n",
    " 'bullshit': 30,\n",
    " 'clown': 40,\n",
    " 'clowns': 29,\n",
    " 'corrupt': 24,\n",
    " 'coward': 30,\n",
    " 'crap': 98,\n",
    " 'crazy': 21,\n",
    " 'damn': 70,\n",
    " 'darn': 14,\n",
    " 'disgusting': 32,\n",
    " 'dumb': 151,\n",
    " 'dumber': 18,\n",
    " 'fool': 138,\n",
    " 'foolish': 36,\n",
    " 'fools': 87,\n",
    " 'fucking': 16,\n",
    " 'garbage': 34,\n",
    " 'gay': 20,\n",
    " 'hypocrisy': 20,\n",
    " 'hypocrite': 66,\n",
    " 'hypocrites': 31,\n",
    " 'hypocritical': 18,\n",
    " 'idiocy': 30,\n",
    " 'idiot': 538,\n",
    " 'idiotic': 101,\n",
    " 'idiots': 340,\n",
    " 'ignorance': 35,\n",
    " 'ignorant': 196,\n",
    " 'imbecile': 33,\n",
    " 'incompetent': 15,\n",
    " 'jerk': 45,\n",
    " 'jerks': 17,\n",
    " 'kill': 16,\n",
    " 'liar': 47,\n",
    " 'loser': 68,\n",
    " 'losers': 50,\n",
    " 'lunatic': 20,\n",
    " 'moron': 165,\n",
    " 'moronic': 19,\n",
    " 'morons': 83,\n",
    " 'nonsense': 14,\n",
    " 'pathetic': 118,\n",
    " 'pig': 17,\n",
    " 'pussy': 24,\n",
    " 'racist': 52,\n",
    " 'ridiculous': 99,\n",
    " 'scum': 43,\n",
    " 'scumbag': 17,\n",
    " 'shit': 53,\n",
    " 'sick': 16,\n",
    " 'silly': 45,\n",
    " 'stupid': 1002,\n",
    " 'stupidest': 17,\n",
    " 'stupidity': 217,\n",
    " 'suck': 15,\n",
    " 'sucks': 21,\n",
    " 'traitor': 16,\n",
    " 'trash': 29,\n",
    " 'troll': 31,\n",
    " 'trolls': 17}\n",
    "\n",
    "tox_dict_key = list(tox_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter or Predicted (either length or toxic word)\n",
    "\n",
    "for i in range(len(only_spans_predicted)):\n",
    "    new_span_sentence = []\n",
    "    new_char_offset_sentence = []\n",
    "    for j in range(len(only_spans_predicted[i])):\n",
    "        if (len(only_spans_predicted[i][j].split(\" \")) >= 5):\n",
    " #           if only_spans_predicted[i][j] in tox_dict_key:\n",
    "            new_span_sentence.append(only_spans_predicted[i][j])\n",
    "            new_char_offset_sentence.append(final_predicted[i][j])\n",
    "        \n",
    "    only_spans_predicted[i] = new_span_sentence\n",
    "    final_predicted[i] = new_char_offset_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for GT (either length or toxic word)\n",
    "\n",
    "for i in range(len(only_spans)):\n",
    "    new_span_sentence = []\n",
    "    new_char_offset_sentence = []\n",
    "    for j in range(len(only_spans[i])):\n",
    "        if  (len(only_spans[i][j].split(\" \")) >= 5) :\n",
    "#            if only_spans[i][j] in tox_dict_key:\n",
    "            new_span_sentence.append(only_spans[i][j])\n",
    "            new_char_offset_sentence.append(final[i][j])\n",
    "        \n",
    "    only_spans[i] = new_span_sentence\n",
    "    final[i] = new_char_offset_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate offsets for Gold (after filter)\n",
    "\n",
    "for i in range(len(final)):\n",
    "    final_answer = []\n",
    "    #print(final[i])\n",
    "    for j in range(len(final[i])):\n",
    "        answer = [number for number in range(final[i][j][0],final[i][j][1] + 1)]\n",
    "        final_answer.extend(answer)\n",
    "    final[i] = final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate offsets for Predicted (after filter)\n",
    "\n",
    "for i in range(len(final_predicted)):\n",
    "    final_answer = []\n",
    "    #print(final[i])\n",
    "    for j in range(len(final_predicted[i])):\n",
    "        answer = [number for number in range(final_predicted[i][j][0],final_predicted[i][j][1] + 1)]\n",
    "        final_answer.extend(answer)\n",
    "    final_predicted[i] = final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f1s = []\n",
    "\n",
    "for i in range(2000):\n",
    "    if (len(final_predicted[i]) == 0) and (len(final[i]) == 0):\n",
    "        continue\n",
    "    else:\n",
    "        f1s.append(f1(final_predicted[i],final[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
